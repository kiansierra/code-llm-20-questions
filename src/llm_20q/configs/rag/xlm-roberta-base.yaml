model_name: "xlm-roberta-base"
output_dir: ../checkpoints/rag/${model_name}

wandb_init:
  job_type: "train-rag"
  tags: ["${model_name}", "RAG"]


input_artifact_version: "v1"
file_name_answers: "openai-answers-${input_artifact_version}.parquet"
input_artifact_answers:
  artifact_or_name: "openai-answers-${input_artifact_version}:latest"
  type: "openai-answers"


file_name_knowledge: "openai-knowledge-${input_artifact_version}.parquet"
input_artifact_knowledge:
  artifact_or_name: "openai-knowledge-${input_artifact_version}:latest"
  type: "openai-knowledge"

output_artifact:
  name: "rag-${model_name}"
  type: model-rag
  metadata: 
    model_name: ${model_name}


trainer:
    # Required parameter:
    output_dir: ${output_dir}
    # Optional training parameters:
    num_train_epochs: 1
    per_device_train_batch_size: 16
    per_device_eval_batch_size: 16
    learning_rate: 2e-5
    warmup_ratio: 0.1
    fp16: True  # Set to False if you get an error that your GPU can't run on FP16
    bf16: False  # Set to True if you have a GPU that supports BF16
    batch_sampler: no_duplicates  # losses that use "in-batch negatives" benefit from no duplicates
    # Optional tracking/debugging parameters:
    eval_strategy: "steps"
    eval_steps: 500
    save_strategy: "steps"
    save_steps: 500
    save_total_limit: 2
    logging_steps: 50