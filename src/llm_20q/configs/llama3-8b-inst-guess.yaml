model_org: meta-llama
model_name: Meta-Llama-3-8B-Instruct
task: guess
output_dir: ../checkpoints/${model_name}/${task}

collator_kwargs:
  response_template: "<|start_header_id|>assistant<|end_header_id|>"
  pad_to_multiple_of: 8


quantization:
  load_in_4bit: True
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: ${dtype:bf16}
  bnb_4bit_use_double_quant: True

model: 
  pretrained_model_name_or_path: ${model_org}/${model_name}
  torch_dtype: ${dtype:bf16}
  attn_implementation: flash_attention_2

wandb_init:
  job_type: "train-sft"
  tags: ["${model_name}", "${task}", "SFT"]

rag_model_name: nomic-embed-text-v1
input_artifact:
  artifact_or_name: "replay-rag-${rag_model_name}:latest"
  type: replay-rag

# input_artifact:
#   artifact_or_name: "replay-dataframe-${task}:latest"
#   type: replay-games

output_artifact:
  name: "sft-${task}-${model_name}"
  type: model-sft
  metadata: 
    model_name: ${model_name}
    task: ${task}

trainer:
  output_dir: ${output_dir}
  overwrite_output_dir: True
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 2
  learning_rate: 2e-4
  save_strategy: steps
  num_train_epochs: 1
  logging_steps: 5
  save_steps: 20
  save_total_limit: 2
  eval_strategy: steps
  eval_steps: 20
  eval_accumulation_steps: 1
  bf16_full_eval: True
  load_best_model_at_end: True
  max_steps: -1
  dataloader_num_workers: 4
  dataloader_persistent_workers: True
  report_to: wandb
  bf16: True
  optim: 'paged_adamw_8bit'
  group_by_length: True
  length_column_name: 'input_length'
  ddp_find_unused_parameters: False

lora:
    r: 32
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']
    bias: "none"
    task_type: "CAUSAL_LM"
